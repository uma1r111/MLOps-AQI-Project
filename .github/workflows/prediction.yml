name: AQI Prediction

on:
  schedule:
    - cron: "0 12 * * *"  # Every day at 5 PM PKT
  workflow_dispatch:         # Allows manual run from GitHub

jobs:
  aqi-prediction:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas numpy scikit-learn statsmodels dvc[s3] bentoml==1.2.0

      - name: Pull feature_selection.csv from S3 (via DVC)
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-1
        run: dvc pull

      - name: Run AQI prediction script
        run: python "Model Training/predict_next_3days.py"

      - name: Track predictions.csv with DVC
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-1
        run: dvc add predictions.csv

      - name: Commit predictions.csv.dvc to GitHub
        run: |
          git config --global user.name "github-actions"
          git config --global user.email "actions@github.com"
          git add predictions.csv.dvc .gitignore
          git commit -m "Update predictions.csv via DVC [skip ci]" || echo "No changes to commit"
          git push origin main

      - name: Push predictions data to S3 (via DVC)
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-1
        run: dvc push

      - name: Get latest SARIMAX BentoML model tag
        id: get_model_tag
        run: |
          # Use --output=json to get full tag without truncation
          MODEL_TAG=$(bentoml models list --output=json | jq -r '.[] | select(.tag | startswith("sarimax_model:")) | .tag' | sort | tail -n1)
          echo "MODEL_TAG=$MODEL_TAG" >> $GITHUB_ENV
          echo "Found model tag: $MODEL_TAG"

      - name: Export SARIMAX model to .bentoml archive
        run: |
          echo "Using model tag: $MODEL_TAG"
          # Extract just the hash part after the colon
          MODEL_HASH=${MODEL_TAG#*:}
          EXPORT_FILENAME="sarimax_model_${MODEL_HASH}.bentoml"
          echo "Exporting to: $EXPORT_FILENAME"
          bentoml models export "$MODEL_TAG" "$EXPORT_FILENAME"
          echo "EXPORT_FILENAME=$EXPORT_FILENAME" >> $GITHUB_ENV

      - name: Upload SARIMAX model to S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-1
        run: |
          echo "Uploading: $EXPORT_FILENAME"
          aws s3 cp "$EXPORT_FILENAME" "s3://s3-bucket-umairrr/models/"

      - name: Clean up older SARIMAX models from S3 (keep last 3)
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-1
        run: |
          echo "Listing all SARIMAX model files in S3..."
          files=$(aws s3 ls s3://s3-bucket-umairrr/models/ | grep sarimax_model_ | sort)
          total_files=$(echo "$files" | wc -l)
          
          echo "Total model files: $total_files"

          if [ "$total_files" -le 3 ]; then
            echo "Nothing to delete. Less than or equal to 3 models."
          else
            to_delete=$(echo "$files" | head -n $(($total_files - 3)) | awk '{print $4}')
            for file in $to_delete; do
              echo "Deleting $file..."
              aws s3 rm "s3://s3-bucket-umairrr/models/$file"
            done
            echo "Cleanup complete."
          fi